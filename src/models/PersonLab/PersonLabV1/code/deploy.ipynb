{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Personlab pose prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the model to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip if model is already registered\n",
    "model = Model.register(model_path = \"./personlab/\",\n",
    "                       model_name = \"personlabV1\",\n",
    "                       description = \"personalab model for detecting poses\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the model is already registered in the workspace fetch it using the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(ws, name='personlabV1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip if model is already downloaded\n",
    "model.download(target_dir='./personlab', exist_ok=False, exists_ok=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry script for deploying on a AKS(Azure Kubernetes Service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_aks.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "sys.path.append('/var/azureml-app/azureml-models/personlabV1/2/personlab')\n",
    "import utils\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def init():\n",
    "    print(time.time())\n",
    "    MODEL_DIR = '/var/azureml-app/azureml-models/personlabV1/2/personlab/'\n",
    "    multiscale = [1.0,1.5,2.0]\n",
    "        \n",
    "    global tf_img\n",
    "    tf_img = []\n",
    "    global outputs \n",
    "    outputs = []\n",
    "    for i in range(len(multiscale)):\n",
    "        scale = multiscale[i]\n",
    "        tf_img.append(tf.placeholder(tf.float32,shape=[1,int(scale*401),int(scale*401),3]))\n",
    "        outputs.append(utils.model(tf_img[i])) \n",
    "    global sess\n",
    "    sess= tf.Session()\n",
    "\n",
    "    global_vars = tf.global_variables()\n",
    "    saver = tf.train.Saver(var_list = global_vars)\n",
    "    checkpoint_path = MODEL_DIR+'model.ckpt'\n",
    "    saver.restore(sess,checkpoint_path)\n",
    "    print(time.time())\n",
    "\n",
    "def run(data):\n",
    "    \n",
    "    try:\n",
    "        print(time.time())\n",
    "       #TODO find logger in \n",
    "        multiscale = [1.0,1.5,2.0]\n",
    "        batch_size,height,width=1,401,401\n",
    "        image_list = json.loads(data)\n",
    "        pose_scoreslist=[]\n",
    "        pose_keypoint_scoreslist=[]\n",
    "        pose_keypoint_coordslist=[]\n",
    "        \n",
    "        \n",
    "        for i in range(1):\n",
    "            if(i==0):\n",
    "                input_image = np.array(image_list['input_image1'], dtype=np.uint8)\n",
    "            else:\n",
    "                input_image = np.array(image_list['input_image2'], dtype=np.uint8)\n",
    "\n",
    "            scale_outputs = []\n",
    "            for i in range(len(multiscale)):\n",
    "                scale = multiscale[i]\n",
    "                cv_shape = (401, 401)\n",
    "                cv_shape2 = (int(cv_shape[0]*scale),int(cv_shape[1]*scale))\n",
    "                scale2=cv_shape2[0]/600\n",
    "                input_img = cv2.resize(input_image,None,fx=scale2,fy=scale2)\n",
    "                #input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "                input_img = cv2.copyMakeBorder(input_img,0,cv_shape2[0]-input_img.shape[0],0,cv_shape2[1]-input_img.shape[1],cv2.BORDER_CONSTANT,value=[127,127,127])\n",
    "                scale_img =input_img\n",
    "                imgs_batch = np.zeros((batch_size,int(scale*height),int(scale*width),3))\n",
    "                imgs_batch[0] = scale_img\n",
    "\n",
    "                one_scale_output = sess.run(outputs[i],feed_dict={tf_img[i]:imgs_batch})\n",
    "                scale_outputs.append([o[0] for o in one_scale_output])\n",
    "\n",
    "            sample_output = scale_outputs[0]\n",
    "            for i in range(1,len(multiscale)):\n",
    "                for j in range(len(sample_output)):\n",
    "                    sample_output[j]+=scale_outputs[i][j]\n",
    "            for j in range(len(sample_output)):\n",
    "                sample_output[j] /=len(multiscale)\n",
    "\n",
    "            H = utils.compute_heatmaps(kp_maps=sample_output[0], short_offsets=sample_output[1])\n",
    "            for i in range(17):\n",
    "                H[:,:,i] = gaussian_filter(H[:,:,i], sigma=2)\n",
    "\n",
    "            pred_kp = utils.get_keypoints(H)\n",
    "            pred_skels = utils.group_skeletons(keypoints=pred_kp, mid_offsets=sample_output[2])\n",
    "            pred_skels = [skel for skel in pred_skels if (skel[:,2]>0).sum() > 6]\n",
    "            #print ('Number of detected skeletons: {}'.format(len(pred_skels)))\n",
    "\n",
    "            pose_scores = np.zeros(len(pred_skels))\n",
    "            pose_keypoint_scores = np.zeros((len(pred_skels), 17))\n",
    "            pose_keypoint_coords = np.zeros((len(pred_skels), 17, 2))\n",
    "\n",
    "            for j in range(len(pred_skels)):\n",
    "                sum=0;\n",
    "                for i in range(17):\n",
    "                    sum+=pred_skels[j][i][2]*100\n",
    "                    pose_keypoint_scores[j][i]=pred_skels[j][i][2]*100\n",
    "                    pose_keypoint_coords[j][i][0]=pred_skels[j][i][0]\n",
    "                    pose_keypoint_coords[j][i][1]=pred_skels[j][i][1]\n",
    "                pose_scores[j]=sum/17\n",
    "                \n",
    "            pose_scoreslist.append(pose_scores)\n",
    "            pose_keypoint_scoreslist.append(pose_keypoint_scores)\n",
    "            pose_keypoint_coordslist.append(pose_keypoint_coords)\n",
    "            \n",
    "\n",
    "        result = json.dumps({'pose_scores': pose_scoreslist, 'keypoint_scores': pose_keypoint_scoreslist,'keypoint_coords':pose_keypoint_coordslist})\n",
    "        # You can return any data type, as long as it is JSON serializable.\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry script to deploy to ACI(Azure Container Instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_aci.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('/var/azureml-app/azureml-models/personlabV1/2/personlab')\n",
    "import utils\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def init(): \n",
    "    MODEL_DIR = '/var/azureml-app/azureml-models/personlabV1/2/personlab/'\n",
    "    multiscale = [1.0,1.5,2.0]\n",
    "        \n",
    "    global tf_img\n",
    "    tf_img = []\n",
    "    global outputs \n",
    "    outputs = []\n",
    "    for i in range(len(multiscale)):\n",
    "        scale = multiscale[i]\n",
    "        tf_img.append(tf.placeholder(tf.float32,shape=[1,int(scale*401),int(scale*401),3]))\n",
    "        outputs.append(utils.model(tf_img[i])) \n",
    "    global sess\n",
    "    sess= tf.Session()\n",
    "\n",
    "    global_vars = tf.global_variables()\n",
    "    saver = tf.train.Saver(var_list = global_vars)\n",
    "    checkpoint_path = MODEL_DIR+'model.ckpt'\n",
    "    saver.restore(sess,checkpoint_path)\n",
    "\n",
    "def run(data):\n",
    "    \n",
    "    try:\n",
    "       #TODO find logger in \n",
    "        multiscale = [1.0,1.5,2.0]\n",
    "        batch_size,height,width=1,401,401\n",
    "        image_list = json.loads(data)\n",
    "        input_image = np.array(image_list['input_image'], dtype=np.uint8)\n",
    "         \n",
    "        scale_outputs = []\n",
    "        for i in range(len(multiscale)):\n",
    "            scale = multiscale[i]\n",
    "            cv_shape = (401, 401)\n",
    "            cv_shape2 = (int(cv_shape[0]*scale),int(cv_shape[1]*scale))\n",
    "            scale2=cv_shape2[0]/600\n",
    "            input_img = cv2.resize(input_image,None,fx=scale2,fy=scale2)\n",
    "            #input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "            input_img = cv2.copyMakeBorder(input_img,0,cv_shape2[0]-input_img.shape[0],0,cv_shape2[1]-input_img.shape[1],cv2.BORDER_CONSTANT,value=[127,127,127])\n",
    "            scale_img =input_img\n",
    "            imgs_batch = np.zeros((batch_size,int(scale*height),int(scale*width),3))\n",
    "            imgs_batch[0] = scale_img\n",
    "\n",
    "            one_scale_output = sess.run(outputs[i],feed_dict={tf_img[i]:imgs_batch})\n",
    "            scale_outputs.append([o[0] for o in one_scale_output])\n",
    "            \n",
    "        sample_output = scale_outputs[0]\n",
    "        for i in range(1,len(multiscale)):\n",
    "            for j in range(len(sample_output)):\n",
    "                sample_output[j]+=scale_outputs[i][j]\n",
    "        for j in range(len(sample_output)):\n",
    "            sample_output[j] /=len(multiscale)\n",
    "            \n",
    "        H = utils.compute_heatmaps(kp_maps=sample_output[0], short_offsets=sample_output[1])\n",
    "        for i in range(17):\n",
    "            H[:,:,i] = gaussian_filter(H[:,:,i], sigma=2)\n",
    "        \n",
    "        pred_kp = utils.get_keypoints(H)\n",
    "        pred_skels = utils.group_skeletons(keypoints=pred_kp, mid_offsets=sample_output[2])\n",
    "        pred_skels = [skel for skel in pred_skels if (skel[:,2]>0).sum() > 6]\n",
    "        #print ('Number of detected skeletons: {}'.format(len(pred_skels)))\n",
    "        \n",
    "        pose_scores = np.zeros(len(pred_skels))\n",
    "        pose_keypoint_scores = np.zeros((len(pred_skels), 17))\n",
    "        pose_keypoint_coords = np.zeros((len(pred_skels), 17, 2))\n",
    "\n",
    "        for j in range(len(pred_skels)):\n",
    "            sum=0;\n",
    "            for i in range(17):\n",
    "                sum+=pred_skels[j][i][2]*100\n",
    "                pose_keypoint_scores[j][i]=pred_skels[j][i][2]*100\n",
    "                pose_keypoint_coords[j][i][0]=pred_skels[j][i][0]\n",
    "                pose_keypoint_coords[j][i][1]=pred_skels[j][i][1]\n",
    "            pose_scores[j]=sum/17\n",
    "            \n",
    "        result = json.dumps({'pose_scores': pose_scores.tolist(), 'keypoint_scores': pose_keypoint_scores.tolist(),'keypoint_coords':pose_keypoint_coords.tolist()})\n",
    "       \n",
    "        # You can return any data type, as long as it is JSON serializable.\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dependencies required for the model and set inference config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment('personlab')\n",
    "env.python.conda_dependencies = CondaDependencies.create(pip_packages=['tensorflow==1.13.2','matplotlib==3.1.1','azureml-defaults', 'scipy==1.3.1', 'inference_schema', 'opencv-python==3.4.5.20', 'scikit-image==0.15.0','scikit-learn==0.21.3', 'psycopg2-binary', 'tqdm', 'Pillow'])\n",
    "inference_config_aci = InferenceConfig(entry_script=\"score_aci.py\", environment=env)\n",
    "inference_config_aks = InferenceConfig(entry_script=\"score_aks.py\", environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use to register the environment to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the deployment config and deploy the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 2, memory_gb = 16,location = \"centralindia\")\n",
    "service = Model.deploy(ws, \"aci-personlabv1\", [model], inference_config_aci, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the service logs if things don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Inference cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For provisioning_configuration(), if you pick custom values for agent_count and vm_size, and cluster_purpose is not DEV_TEST, then you need to make sure agent_count multiplied by vm_size is greater than or equal to 12 virtual CPUs. For example, if you use a vm_size of \"Standard_D3_v2\", which has 4 virtual CPUs, then you should pick an agent_count of 3 or greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip if already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Use the default configuration (you can also provide parameters to customize this).\n",
    "# For example, to create a dev/test cluster, use:\n",
    "# prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "aks_name = 'webservices'\n",
    "\n",
    "prov_config = prov_config = AksCompute.provisioning_configuration(vm_size = \"Standard_A2_v2\",\n",
    "                                                       agent_count = 6,\n",
    "                                                       location = \"centralindia\")\n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.compute import AksCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model on created AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_target = AksCompute(ws,\"webservices\")\n",
    "\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 6, memory_gb = 64,autoscale_enabled=True, autoscale_max_replicas=3, collect_model_data=True, enable_app_insights=True)\n",
    "service = Model.deploy(ws, \"aks-personlab-eur-test1\", [model], inference_config_aks, deployment_config,aks_target)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
