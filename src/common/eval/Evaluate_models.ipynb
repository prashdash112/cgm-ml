{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a trained model\n",
    "\n",
    "## Setup\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "import glob2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent / 'src'))\n",
    "\n",
    "from eval_utils import calculate_performance, CODE_TO_SCANTYPE, CONFIG, REPO_DIR, preprocess_targets, preprocess_depthmap, tf_load_pickle, preprocess, extract_qrcode, extract_scantype, avgerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the  model to be evaluated from workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "\n",
    "selected_experiments = [\"q3-depthmap-plaincnn-height-95k\"]\n",
    "RUN_ID = 'q3-depthmap-plaincnn-height-95k_1597988908_42c4ef33'  # Run3\n",
    "OUTPUT_DIR = 'data/logs/q3-depthmap-plaincnn-height-95k/run_03/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the models on your local system for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'evaluation_95k_30082020/q3-depthmap-plaincnn-height-100-95k/run_03/outputs/best_model.h5'\n",
    "MODEL_PATH = str(REPO_DIR / \"data/outputs/best_model_Run3_nodropout.h5\")\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "# summarize model.\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a sample from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = '../testdepthmap1/1585551618-hlby208u8z/pc_1585551618-hlby208u8z_1593156356859_100_000.p'\n",
    "paths = REPO_DIR / \"data/anon-depthmap-testset/scans/1585551618-hlby208u8z/100/pc_1585551618-hlby208u8z_1593156356859_100_000.p\"\n",
    "\n",
    "depthmap, targets = pickle.load(open(paths, \"rb\"))\n",
    "depthmap = preprocess_depthmap(depthmap)\n",
    "depthmap = depthmap / depthmap.max()\n",
    "print(\"depthmap_max:\", depthmap.max())\n",
    "depthmap = tf.image.resize(depthmap, (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH))\n",
    "targets = preprocess_targets(targets, CONFIG.TARGET_INDEXES)\n",
    "depthmap.set_shape((CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1))\n",
    "# targets.set_shape((len(targets_indices,)))\n",
    "plt.imshow(np.squeeze(depthmap), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To use the dataset, you can:\n",
    "- mount the dataset\n",
    "- use datastore (blob storage)\n",
    "- download the dataset\n",
    "\n",
    "Choose your preferred way and make sure to adjust the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '/mnt/depthmap/depthmap_testset/scans/*/*/'\n",
    "DATASET_PATH = str(REPO_DIR / \"data/anon-depthmap-testset/scans/*/*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = glob2.glob(DATASET_PATH); prediction_folder[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = prediction_folder[:10]  # reduce size for DEBUG speed\n",
    "\n",
    "predictions = []\n",
    "for qrcode in tqdm(prediction_folder):\n",
    "    depthmaps_pred = []\n",
    "    labels = []\n",
    "    depthfiles = []\n",
    "    depthmaps = glob2.glob(qrcode + '/*.p')\n",
    "    for files in depthmaps:\n",
    "        depths, targets = preprocess(files)\n",
    "        depthmaps_pred.append(depths)\n",
    "        labels.append(targets)\n",
    "        depthfiles.append(files)\n",
    "    files_to_predict = tf.stack(depthmaps_pred)\n",
    "    inference = model.predict(files_to_predict)\n",
    "    predictions.append([qrcode, depthfiles,np.squeeze(inference), labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put predictions into dataframe\n",
    "df = pd.DataFrame([])\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    label = np.array(predictions[i][3]).flatten()\n",
    "    data = pd.DataFrame({\n",
    "        'qrcode':predictions[i][0],\n",
    "        'artifacts': predictions[i][1],\n",
    "        'predicted':predictions[i][2],\n",
    "        'GT':label,\n",
    "    })\n",
    "    df = df.append(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qrcode'] = df.apply(extract_qrcode, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[1]  # sample of how the artifacts path looks like for me, modify it accordingly to suit your path dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['qrcode'].unique()) ## total number of scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scantype'] = df.apply(extract_scantype, axis=1)\n",
    "df['scantype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the results of artifacts by qrcode and scantype by taking mean across the same scantype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = df.groupby(['qrcode', 'scantype']).mean()\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error between predicted and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE['error'] = MAE.apply(avgerror, axis=1)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique name for the index values\n",
    "model_name = 'q3-depthmap-plaincnn-height-100-95k'\n",
    "run_no ='_front_run_03'\n",
    "complete_name = model_name + run_no; complete_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracies across the scantypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for code in CODE_TO_SCANTYPE.keys():\n",
    "    df = calculate_performance(code, MAE)\n",
    "    full_model_name = complete_name + CODE_TO_SCANTYPE[code]\n",
    "    df.rename(index={0:full_model_name}, inplace=True)\n",
    "    display(HTML(df.to_html()))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the results for all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs)\n",
    "result.index.name = 'Model_Scantype'\n",
    "result = result.round(2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model results in csv file\n",
    "CSV_OUT_PATH = REPO_DIR / 'data' / 'eval' / RUN_ID / 'result.csv'\n",
    "Path(CSV_OUT_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "result.to_csv(CSV_OUT_PATH, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
